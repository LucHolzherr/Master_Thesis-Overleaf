\chapter{Introduction}
\label{sec:introduction}
%\chapter{Einleitung}
%\label{sec:einleitung}

In this master thesis project a robotic object search and delivery problem is studied and different solving methods evaluated. Object search problems are interesting for academia as the agent has to perform reasoning with incomplete information in a real environment with large state, action and observation space. A possible future application of the studied methods is a service robot that autonomously serves coffee in a restaurant or an office.  \\

The task takes place in a known indoor environment and involves multiple items. The agent is given prior knowledge about the item locations which it should use to find and deliver the items in as little time as possible. A low fidelity simulation was created from scratch to develop, test and compare different methodological approaches. The task is modelled as a partially observable Markov decision process (POMDP). Solving POMDPs is computationally demanding and intractable for large problems. To decrease the computational burden a novel hierarchical Multi-Scale POMDP framework was developed that can solve object search and delivery tasks in a large office environment. Three different Multi-Scale agents are presented and their performance is compared in simulation.\\

The Multi-Scale POMDP approach presented is not bound to object search and delivery tasks only. Any (PO)MDP problem where the state space includes discretised physical space could be adapted to the MultiScale framework. 



\section{Thesis Overview}
The thesis is structured into five chapters. In section \ref{sec:relatedwork} related work in the field of hierarchical POMDP models and object search methods are presented. The next chapter covers the underlying concepts of (partially observable) Markov decision processes which build the foundation of this thesis. In chapter \ref{sec:object_search} the object search methods explored in this thesis are discussed in more details. Chapter \ref{sec:results} presents the conducted experiments and gives a quantitative evaluation regarding computation speed and solution quality of the proposed methods. Finally, chapter \ref{sec:conclusion} summarizes the main findings and give an outlook for further research on Multi-Scale POMDPs. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Work}\label{sec:relatedwork}
Partially observable Markov decision processes are a powerful tool for planning under uncertainty. They describe a mathematical framework for sequential decision problems with uncertain states in stochastic environments \cite{10.2307/168926}. Solving POMDps exactly \cite{KAELBLING199899} is computationally intractable for real-world problems. Instead, anytime algorithms which approximate the optimal solution through sampling are used. They can be divided in two major groups, point-based methods and Monte-Carlo methods. Point-based methods \cite{10.5555/1630659.1630806, Spaan_2005, 6284837} accelerate the solving process by sampling a small set of points from the belief space and approximate the value function through a set of $\alpha$-vectors. They are offline solvers, i.e., the output is a value function which can be used to compute a policy. In this thesis SARSOP \cite{6284837}, a state-of-the-art point-based method is used. The Monte-Carlo methods \cite{NIPS2010_4031, NIPS2013_5189, Bai2011} are online solvers. They perform a lookahead search on a belief tree to find the best next action to execute. They can also be adapted for problems with continuous state space \cite{Bai2011}.\\

An other strand of literature aims at modifying the problem structure in order to get speedups in computation time. R. S. Sutton et al. extended the MDP framework to include options \cite{SUTTON1999181}, which are sometimes referred to as macro-actions. An option is an action that takes multiple time-steps to execute and solves a subproblem of the task like picking up an item.  An option is typically a closed-loop policy and can be added as separate actions to the action space of the MDP. This can speedup the solving process as an option leads to large state transitions. However, the state space considered remains identical to the original MDP problem which limits the computational speedup. M. Hauskrecht et al. further investigated the option framework and came up with the idea of abstract MDP\cite{DBLP:journals/corr/abs-1301-7381}. They divided the state space into regions and introduced entry and exit states to the regions called abstract states. The macro actions are used to transition in between abstract states. Both state and action space of the abstract MDP is smaller than in the original MDP. This reduction yields a large computational speedup at the cost of computing suboptimal solutions. The applications of the abstract MDPs seem to be limited to problems where the state space can be divided into non-interacting regions. A 


references solvers:
\begin{itemize}
    \item\cite{10.2307/168926} first POMDP formulation
    \item \cite{KAELBLING199899} exact solution, adapted value iteration
    \item \cite{10.5555/1630659.1630806} point based method 2003
    \item \cite{Spaan_2005} perseus, point based
    \item \cite{6284837} SARSOP
    \item \cite{NIPS2013_5189} DESPOT
    \item \cite{NIPS2010_4031} Monte carlo
    \item\cite{Bai2011} Monte carlo value iteration continuous states
\end{itemize}
refernces hierarchical methods:
\begin{itemize}
    \item \cite{SUTTON1999181} option framework
    \item \cite{DBLP:journals/corr/abs-1301-7381} MDP macro actions abstract states
    \item \cite{Pineau02anintegrated} POMDP task hierarchy
    \item \cite{FOKA2007561} real-time hierarchical
    \item \cite{rmax} rL rmax+maxQ
    \item \cite{Gopalan2017PlanningWA} abstract MDP
    \item \cite{Lanighan2018PlanningRM} hierarchical belief spaces
    \item \cite{7140035} belief space macro-actions
\end{itemize}
 