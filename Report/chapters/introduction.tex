\chapter{Introduction}
\label{sec:introduction}
%\chapter{Einleitung}
%\label{sec:einleitung}

In this master thesis project a robotic object search and delivery problem is studied and different solving methods evaluated. The task takes place in a known indoor environment and involves multiple items. The agent is given prior knowledge about the item locations in form of a probability distribution which it should use to find and deliver the items in as little time as possible. An application of the studied methods is a general service robot for a restaurant or an office that autonomously fetches and delivers items, such as coffee.\\

Object search problems are interesting as the agent has to perform reasoning with incomplete information in a real environment with large state, action and observation spaces. The goal of this thesis is to develop a high-level planning algorithm that solves object search and delivery tasks in a large office environment with many rooms.\\

 A low fidelity simulation was created from scratch to develop, test and compare different methodological approaches. The task is modelled as a partially observable Markov decision process (POMDP). Solving POMDPs is computationally demanding and intractable for large problems. To decrease the computational burden a novel hierarchical Multi-Scale POMDP framework was developed that can solve object search and delivery tasks in a large office environment. Three different Multi-Scale agents are presented and their performance is compared in simulation.\\

The Multi-Scale POMDP approach presented is not bound to object search and delivery tasks only. Any MDP or POMDP problem where the state space includes discretised physical space could be adapted to the Multi-Scale framework. 



\section{Thesis Overview}
The thesis is structured into five chapters. In Section \ref{sec:relatedwork} related work in the field of POMDP frameworks are presented. The next chapter covers the underlying concepts of (partially observable) Markov decision processes which build the foundation of this thesis. In Chapter \ref{sec:object_search} the object search methods explored in this thesis are discussed in more details. Chapter \ref{sec:results} presents the conducted experiments and gives a quantitative evaluation regarding computation speed and solution quality of the proposed methods. Finally, Chapter \ref{sec:conclusion} summarizes the main findings and give an outlook for further research on Multi-Scale POMDPs. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Related Work}\label{sec:relatedwork}
Partially observable Markov decision processes are a powerful tool for planning under uncertainty. They describe a mathematical framework for sequential decision problems with uncertain states in stochastic environments \cite{10.2307/168926}. Solving POMDPs exactly is computationally intractable for real world problems \cite{KAELBLING199899}. Instead, anytime algorithms which approximate the optimal solution through sampling are used. They can be divided in two major groups, \textit{point-based} methods and \textit{Monte-Carlo} methods. Point-based methods accelerate the solving process by sampling a small set of points from the belief space and approximate the value function through a set of $\alpha$-vectors \cite{10.5555/1630659.1630806, Spaan_2005, 6284837}. They are offline solvers, i.e., the output is a value function which can be used to compute a policy. In this thesis SARSOP \cite{6284837}, a state-of-the-art point-based method is used. The Monte-Carlo methods are online solvers \cite{NIPS2010_4031, NIPS2013_5189, Bai2011}. They perform a lookahead search on a belief tree to find the best next action to execute. They can also be adapted for problems with continuous state space \cite{Bai2011}.\\

Another strand of literature aims at modifying the problem structure in order to decrease the computational burden. The novel Multi-Scale approach presented in this thesis falls into this category. R. S. Sutton et al. extended the MDP framework to include \textit{options} \cite{SUTTON1999181}, sometimes referred to as macro-actions. An option is an action that takes multiple time-steps to execute and solves a subproblem of the task like picking up an item.  An option is typically a closed-loop policy and can be added as separate actions to the action space of the MDP. This can speedup the solving process as an option leads to large state transitions. However, the state space considered remains identical to the original MDP problem which limits the computational speedup. The option framework can be extended to POMDPs where an option is a closed-loop policy acting on the belief space as is shown in the work of S. Omidshafiei et al. \cite{7140035}.\\

M. Hauskrecht et al. further researched the option framework and came up with the idea of an \textit{abstract} MDP \cite{DBLP:journals/corr/abs-1301-7381}. They divided the state space into regions and introduced entry and exit states to the regions called abstract states. The macro actions are used to transition in between abstract states. Both state and action space of the abstract MDP is smaller than in the original MDP. This reduction yields a large computational speedup at the cost of suboptimal solutions. The applications of the abstract MDPs also seem to be limited to problems where the state space can be divided into non-interacting regions.\\

There are multiple papers on hierarchical (PO)MDPs where the original problem is partitioned into smaller subproblems \cite{Pineau02anintegrated, rmax, Gopalan2017PlanningWA}.  They create an \textit{action hierarchy}, where each higher layer action is a (PO)MDP on its own. Those methods can lead to faster computation times and only small solution quality losses if the original POMDP can be split into almost unrelated subproblems. The higher layer transition, reward and observation models can be computed by either first solving the hierarchy from the bottom up \cite{Pineau02anintegrated, rmax} or by using heuristics \cite{Gopalan2017PlanningWA}. The former approach requires solving many POMDP problems which reduces the computational speedup and for the later approach it can be difficult to find good heuristics. In this thesis the higher layer models are created through fixed policies on the lower layers. The work of N. Gopalan et al. \cite{Gopalan2017PlanningWA} is interesting as they use abstract states in addition to the task hierarchy. An abstract state summarizes multiple states of the original MDP. The state abstraction further reduces computation time at the cost of losing major reasoning abilities and therefore poor solution quality. This thesis explores a similar hierarchical approach but achieves better reasoning abilities.\\
Another approach related to my work is the hierarchical POMDP approach from A. Foka and P. Trahanias who explored a state space hierarchy with different resolution levels for autonomous robot navigation \cite{FOKA2007561}. However, their hierarchical layout is very specific to robotic navigation and difficult to generalize to other problems. 
